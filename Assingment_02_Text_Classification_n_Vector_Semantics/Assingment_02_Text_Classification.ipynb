{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 02<br>\n",
    "## RUHANI FAIHEEM RAHMAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the 4 entity to get precision, recall, accuracy, f_measure\n",
    "def calculate_all(y_pred, y_true):\n",
    "    true_positive = 0\n",
    "    false_positive = 0\n",
    "    false_negetive = 0\n",
    "    true_negetive = 0\n",
    "\n",
    "    for i in zip(y_pred, y_true):\n",
    "        if i[0] == i[1]:\n",
    "            if i[0] == 1:\n",
    "                true_positive += 1\n",
    "            else: \n",
    "                true_negetive += 1\n",
    "        else:\n",
    "            if i[0] == 1:\n",
    "                false_positive += 1\n",
    "            else: \n",
    "                false_negetive += 1\n",
    "    return true_positive, false_positive, false_negetive, true_negetive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y_pred, y_true):\n",
    "    true_positive, false_positive, false_negetive, true_negetive = calculate_all(y_pred, y_true)\n",
    "    return true_positive * 1.0 / (true_positive + false_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y_pred, y_true):\n",
    "    true_positive, false_positive, false_negetive, true_negetive = calculate_all(y_pred, y_true)\n",
    "    return true_positive * 1.0 / (true_positive + false_negetive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_true):\n",
    "    true_positive, false_positive, false_negetive, true_negetive = calculate_all(y_pred, y_true)\n",
    "    return (true_positive + true_negetive) * 1.0 / (len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_measure(precision, recall):\n",
    "    return (2.0 * precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate All Score Together\n",
    "def get_all_score(y_pred, y_true):\n",
    "    __precision =  precision(y_pred, y_true)\n",
    "    __recall = recall(y_pred, y_true)\n",
    "    __accuracy = accuracy(y_pred, y_true)\n",
    "    __f_measure = f_measure(__precision, __recall)\n",
    "    print('Precision\\t=\\t{:.4f}\\nRecall\\t\\t=\\t{:.4f}\\nAccuracy\\t=\\t{:.4f}\\nF1 Score\\t=\\t{:.4f}\\n'.format(__precision, __recall, __accuracy, __f_measure))\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all files from a directory and return the content as a list\n",
    "def read_files(directory):\n",
    "    data_review = []\n",
    "    filenames = os.listdir(directory)\n",
    "    total_files = len(filenames)\n",
    "    count = 0\n",
    "    for filename in filenames:\n",
    "        if count >= total_files/1.0:\n",
    "            break\n",
    "        count += 1\n",
    "        with open(os.path.join(directory, filename),'r', encoding='utf-8') as file:\n",
    "            data_review.append(file.read())\n",
    "    return data_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "home_directory = 'data/aclImdb'\n",
    "train_directory = os.path.join(home_directory, \"train\")\n",
    "test_directory = os.path.join(home_directory, \"test\")\n",
    "\n",
    "train_positive_directory = os.path.join(train_directory, \"pos\")\n",
    "train_negetive_directory = os.path.join(train_directory, \"neg\")\n",
    "\n",
    "test_positive_directory = os.path.join(test_directory, \"pos\")\n",
    "test_negetive_directory = os.path.join(test_directory, \"neg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magority class baseline classifier calculation\n",
    "def majority_class_baseline_classifier(positive, negetive):\n",
    "    import numpy as np\n",
    "    return np.concatenate((np.ones(len(positive), dtype = int), np.ones(len(negetive), dtype = int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True classifier\n",
    "def true_class_baseline_classifier(positive, negetive):\n",
    "    import numpy as np\n",
    "    return np.concatenate((np.ones(len(positive), dtype = int), np.zeros(len(negetive), dtype = int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to find out the majority based classifer\n",
    "def majority_baseline(positive_data, negetive_data, classifier_text):\n",
    "    y_pred = majority_class_baseline_classifier(positive_data, negetive_data)\n",
    "    y_true = true_class_baseline_classifier(positive_data, negetive_data)\n",
    "    print(classifier_text)\n",
    "    get_all_score(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_positive_data_list = read_files(train_positive_directory)\n",
    "train_negetive_data_list = read_files(train_negetive_directory)\n",
    "test_positive_data_list = read_files(test_positive_directory)\n",
    "test_negetive_data_list = read_files(test_negetive_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Based Classifier: Train\n",
      "Precision\t=\t0.5000\n",
      "Recall\t\t=\t1.0000\n",
      "Accuracy\t=\t0.5000\n",
      "F1 Score\t=\t0.6667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "majority_baseline(train_positive_data_list,train_negetive_data_list,'Majority Based Classifier: Train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Based Classifier: Train\n",
      "Precision\t=\t0.5000\n",
      "Recall\t\t=\t1.0000\n",
      "Accuracy\t=\t0.5000\n",
      "F1 Score\t=\t0.6667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "majority_baseline(test_positive_data_list,test_negetive_data_list,'Majority Based Classifier: Train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores above are calculated for the majority based classifier. We use the positive movie review as the majority based classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_length_classifier(review_data, length):\n",
    "    class_pred = list()\n",
    "    for reivew in review_data:\n",
    "        word_count = len(reivew.split(' '))\n",
    "        if word_count > length:\n",
    "            class_pred.append(1)\n",
    "        else:\n",
    "            class_pred.append(0)\n",
    "    return class_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_based_classifier(positive, negetive, threshold_length):\n",
    "    import numpy as np\n",
    "    \n",
    "    positive_pred = review_length_classifier(positive, threshold_length)\n",
    "    negetive_pred = review_length_classifier(positive, threshold_length)\n",
    "\n",
    "    return np.concatenate([positive_pred, negetive_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length Based Classifier - 50 : Train Data\n",
      "\n",
      "Precision\t=\t0.5000\n",
      "Recall\t\t=\t0.9720\n",
      "Accuracy\t=\t0.5000\n",
      "F1 Score\t=\t0.6603\n",
      "\n",
      "Length Based Classifier - 100 : Train Data\n",
      "\n",
      "Precision\t=\t0.5000\n",
      "Recall\t\t=\t0.8690\n",
      "Accuracy\t=\t0.5000\n",
      "F1 Score\t=\t0.6348\n",
      "\n",
      "Length Based Classifier - 250 : Train Data\n",
      "\n",
      "Precision\t=\t0.5000\n",
      "Recall\t\t=\t0.3087\n",
      "Accuracy\t=\t0.5000\n",
      "F1 Score\t=\t0.3817\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for length in (50, 100, 250):\n",
    "    y_pred = length_based_classifier(train_positive_data_list, train_negetive_data_list, length)\n",
    "    y_true = true_class_baseline_classifier(train_positive_data_list, train_negetive_data_list)\n",
    "\n",
    "    print('Length Based Classifier - {} : Train Data\\n'.format(length))\n",
    "    get_all_score(y_pred, y_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length Based Classifier - 50 : Test Data\n",
      "\n",
      "Precision\t=\t0.5000\n",
      "Recall\t\t=\t0.9721\n",
      "Accuracy\t=\t0.5000\n",
      "F1 Score\t=\t0.6603\n",
      "\n",
      "Length Based Classifier - 100 : Test Data\n",
      "\n",
      "Precision\t=\t0.5000\n",
      "Recall\t\t=\t0.8646\n",
      "Accuracy\t=\t0.5000\n",
      "F1 Score\t=\t0.6336\n",
      "\n",
      "Length Based Classifier - 250 : Test Data\n",
      "\n",
      "Precision\t=\t0.5000\n",
      "Recall\t\t=\t0.2866\n",
      "Accuracy\t=\t0.5000\n",
      "F1 Score\t=\t0.3643\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for length in (50, 100, 250):\n",
    "    y_pred = length_based_classifier(test_positive_data_list, test_negetive_data_list, length)\n",
    "    y_true = true_class_baseline_classifier(test_positive_data_list, test_positive_data_list)\n",
    "\n",
    "    print('Length Based Classifier - {} : Test Data\\n'.format(length))\n",
    "    get_all_score(y_pred, y_true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we classified the review based on the movie length 50,100,250. We tried the length with 500 and 1000 but our recall and f1 score decreases with the increase of the word length. So, we pick 50, 100 and 250. You can obseve that with the increase of length, recall and F1 score drastically changes to downward. However test and traing socres are pretty simmilar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using 25000 dataset to train and test the model consumes huge amount of time and most of the cases the system crashes. Thats why we use a smaller dataset, train and test our model according to the reduced size dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Method to reduce the data volume. As, my PC can't process 25000 input data, \n",
    "# So I reduced it for better understanding the features and process.\n",
    "# 2 * n is the total number of training set.\n",
    "\n",
    "def reduce_feat_data(file_name, n):\n",
    "    with open(file_name,'r') as file:\n",
    "        data = file.readlines()\n",
    "#     print(len(data))\n",
    "#     data = data[:1000]\n",
    "    pos_count = 0\n",
    "    neg_count = 0\n",
    "    with open(file_name+'_reduced','w+') as file:\n",
    "        for __ in data:\n",
    "            if int(__.split(' ')[0]) > 5:\n",
    "                if pos_count == n:\n",
    "                    continue\n",
    "                pos_count += 1\n",
    "                file.write(__)\n",
    "            else:\n",
    "                if neg_count == n:\n",
    "                    continue\n",
    "                neg_count += 1\n",
    "                file.write(__)\n",
    "                \n",
    "    return\n",
    "reduce_feat_data(os.path.join(train_directory, \"labeledBow.feat\"), 2500)\n",
    "reduce_feat_data(os.path.join(test_directory, \"labeledBow.feat\"), 2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load svmlight files\n",
    "def load_files(files):\n",
    "    from sklearn.datasets import load_svmlight_files\n",
    "    return load_svmlight_files(files, n_features=None, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If rating is higher than the threshold, that's positive class, otherwise negetive class\n",
    "def review_length_classifier(y, rating):\n",
    "    class_pred = list()\n",
    "    for __ in y:\n",
    "        if __ >= rating:\n",
    "            class_pred.append(1)\n",
    "        else:\n",
    "            class_pred.append(0)\n",
    "    return class_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing data and reduced the graph to_dence for better performance\n",
    "def process_data(files):\n",
    "    x_train, y_train, x_test, y_test = load_files(files)\n",
    "\n",
    "    x_train = np.int16(x_train.todense())\n",
    "    y_train = review_length_classifier(y_train,5)\n",
    "    \n",
    "    x_test = np.int16(x_test.todense())\n",
    "    y_test = review_length_classifier(y_test,5)\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature = os.path.join(train_directory, \"labeledBow.feat_reduced\")\n",
    "test_feature = os.path.join(test_directory, \"labeledBow.feat_reduced\")\n",
    "files = [train_feature, test_feature]\n",
    "x_train, y_train, x_test, y_test = process_data(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GaussianNB Model\n",
    "model = GaussianNB()\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Classifier: Train Data\n",
      "\n",
      "Precision\t=\t0.9962\n",
      "Recall\t\t=\t0.9564\n",
      "Accuracy\t=\t0.9764\n",
      "F1 Score\t=\t0.9759\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = model.predict(x_train)\n",
    "print('Naive Bayes Classifier: Train Data\\n')\n",
    "get_all_score(y_pred_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Classifier: Test Data\n",
      "\n",
      "Precision\t=\t0.6057\n",
      "Recall\t\t=\t0.4712\n",
      "Accuracy\t=\t0.5822\n",
      "F1 Score\t=\t0.5300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_test = model.predict(x_test)\n",
    "print('Naive Bayes Classifier: Test Data\\n')\n",
    "get_all_score(y_pred_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Socres with both Train and Test data. It is clearly seen that train data scores are far better than the test scores. The reason is train data overfits the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='auto',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='newton-cg',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic Regression Model\n",
    "# newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘sag\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(solver='newton-cg', multi_class='auto')\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Classifier: Train Data\n",
      "\n",
      "Precision\t=\t1.0000\n",
      "Recall\t\t=\t1.0000\n",
      "Accuracy\t=\t1.0000\n",
      "F1 Score\t=\t1.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = model.predict(x_train)\n",
    "print('Logistic Regression Classifier: Train Data\\n')\n",
    "get_all_score(y_pred_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Classifier: Test Data\n",
      "\n",
      "Precision\t=\t0.8619\n",
      "Recall\t\t=\t0.8116\n",
      "Accuracy\t=\t0.8408\n",
      "F1 Score\t=\t0.8360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_test = model.predict(x_test)\n",
    "print('Logistic Regression Classifier: Test Data\\n')\n",
    "get_all_score(y_pred_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Socres with both Train and Test data. It is clearly seen that train data scores are far better than the test scores. The reason is train data overfits the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 06"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the comparision for all models,<br>\n",
    "Baseline Classifier<br>\n",
    "F1 Score for each train and test is 0.67 which is same.<br>\n",
    "Length Base Classifier<br>\n",
    "Score for train and test is 0.66, 0.63, 0.38 for train. which is lower than the previoous one. Test set score of review length over 250 is .34 is lower than the train set.<br>\n",
    "Naive Bayes Classifier<br>\n",
    "Naive bayes train set socre is close to 1 but test set accuracy is 0.58 which is pretty lower. That means the model does not fit well.<br>\n",
    "Logistic Regression Classifier<br>\n",
    "Logistic Regression score for train is 1.00 but test score is 0.81 which is affordable, So this model fits better than other models. We also try with different solver for Logistic Regression for example, newton-cg, lbfgs, liblinear, sag. All the solver output is pretty much closer. So, we chose newto-cg.<br>\n",
    "Finally, we can conclude by saying that logitic regression model fits well and predicts well in our data set. Even though it overfits in the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 7:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google word2vec model\n",
    "filename = 'data/GoogleNews-vectors-negative300.bin'\n",
    "google_word2vec_model = KeyedVectors.load_word2vec_format(filename, binary=True, limit=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOVE model\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "word2vec_output = 'data/glove.42B.300d.word2vec.txt'\n",
    "glove_file = 'data/glove.42B.300d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1917494, 300)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# changing GLOVE format to word2vec format\n",
    "glove2word2vec(glove_file, word2vec_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOVE model\n",
    "glove_model = KeyedVectors.load_word2vec_format(word2vec_output, binary=False, limit=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloading test data\n",
    "def download_test_file(link):\n",
    "    import requests\n",
    "    response = requests.get(link)\n",
    "    return response.text.split('\\n')[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write test data to file\n",
    "def write_file(file_name, data):\n",
    "    file = open(file_name,'w+')\n",
    "    for d in data:\n",
    "        file.write(d+'\\n')\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read test from the file\n",
    "def read_file(file_name):\n",
    "    with open(file_name,'r') as file:\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Preprocessing the data based on the analogy. Creating dictionary of the analogy.\n",
    "    This dictionay will contain the analogy as keys as list of test as the values.\n",
    "    We also use lower case for test data when using GLOVE, Beacuse GLOVE doest not contain the Capital words.\n",
    "'''\n",
    "\n",
    "def preprocess_data(data, key_list, isLower):\n",
    "    pre_dict = dict()\n",
    "    data = data.split(':')\n",
    "#     print(data[3])\n",
    "    for lists in data:\n",
    "#         print(lists)\n",
    "        key_split = lists.split('\\n')\n",
    "        if key_split[0].strip() in key_list:\n",
    "            key = key_split[0].strip()\n",
    "            pre_dict[key] = dict()\n",
    "            temp_list = list()\n",
    "            for i in range (1, len(key_split)):\n",
    "                raw_value = key_split[i].split(' ')\n",
    "                if isLower:\n",
    "                    raw_value = [word.lower() for word in raw_value]\n",
    "#                 print(raw_value)\n",
    "                if (len(raw_value)==4):\n",
    "                    raw_dict = {'a':raw_value[0],'b':raw_value[1],'c':raw_value[2],'d':raw_value[3]}\n",
    "                    temp_list.append(raw_dict)\n",
    "            pre_dict[key] = temp_list\n",
    "    return pre_dict\n",
    "\n",
    "# pre_dict = preprocess_data(data, key_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making the evaluation dictionary\n",
    "def create_evaluation_dict(key_list):\n",
    "    temp_dict = dict()\n",
    "    for key in key_list:\n",
    "        temp_dict[key] = 0\n",
    "    return temp_dict\n",
    "# evaluation_dict = create_evaluation_dict(key_list)\n",
    "# evaluation_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test using the embedding model\n",
    "\n",
    "def evaluate_test(model,data,evaluation_dict,isOutputAll):\n",
    "    for key, val in data.items():\n",
    "        list_val = data[key]\n",
    "        pos = 0\n",
    "        neg = 0\n",
    "        for each_test in list_val:\n",
    "            if all(isExist in model for isExist in [each_test['a'],each_test['b'],each_test['c']]):\n",
    "                predict = model.most_similar(positive=[each_test['b'],each_test['c']], negative=[each_test['a']], topn=1)                \n",
    "                if isOutputAll:\n",
    "                    print('Predicted: {}\\t\\tTrue Value: {}'.format(str(predict),each_test['d']))\n",
    "                if predict[0][0] == each_test['d']:\n",
    "                    pos += 1\n",
    "                else:\n",
    "                    neg += 1\n",
    "            else:\n",
    "                continue\n",
    "#                 print(each_test)\n",
    "        try:            \n",
    "            evaluation_dict[key] = pos * 100.0 / (pos + neg)\n",
    "        except:\n",
    "            print('Divide by Zero Occured for key: ', key)\n",
    "            evaluation_dict[key] = 0.0\n",
    "    print(\"Accuracy By Analogy:\")\n",
    "    for key, val in evaluation_dict.items():\n",
    "        print(\"{}\\t\\t{:.2f} %\".format(key,val))\n",
    "    return evaluation_dict\n",
    "# evaluate_test(google_word2vec_model,pre_dict,evaluation_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this method will prints the time taken for testing.\n",
    "def test_model(model,pre_dict,evaluation_dict, isOutputAll):\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    result = evaluate_test(model,pre_dict,evaluation_dict, isOutputAll)\n",
    "    end_time = time.time()\n",
    "    print(\"--- %s seconds ---\" % (end_time - start_time))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test with Google Word2Vec Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'word-test.v1.txt'\n",
    "data = read_file(file_name)\n",
    "key_list = ['capital-world', \n",
    "    'currency', \n",
    "    'city-in-state', \n",
    "    'family',\n",
    "    'gram1-adjective-to-adverb', \n",
    "    'gram2-opposite', \n",
    "    'gram3-comparative',\n",
    "    'gram6-nationality-adjective']\n",
    "pre_dict = preprocess_data(data, key_list, False)\n",
    "evaluation_dict = create_evaluation_dict(key_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Word2Vec Model Accuacy on the Analogy Test Data\n",
      "Accuracy By Analogy:\n",
      "capital-world\t\t46.54 %\n",
      "currency\t\t32.43 %\n",
      "city-in-state\t\t72.19 %\n",
      "family\t\t85.18 %\n",
      "gram1-adjective-to-adverb\t\t30.44 %\n",
      "gram2-opposite\t\t43.37 %\n",
      "gram3-comparative\t\t91.14 %\n",
      "gram6-nationality-adjective\t\t90.13 %\n",
      "--- 63.90122175216675 seconds ---\n"
     ]
    }
   ],
   "source": [
    "print(\"Google Word2Vec Model Accuacy on the Analogy Test Data\")\n",
    "result = test_model(google_word2vec_model,pre_dict,evaluation_dict, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test with Glove Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLOVE Model Accuacy on the Analogy Test Data\n",
      "Accuracy By Analogy:\n",
      "capital-world\t\t52.42 %\n",
      "currency\t\t11.92 %\n",
      "city-in-state\t\t78.07 %\n",
      "family\t\t90.48 %\n",
      "gram1-adjective-to-adverb\t\t30.24 %\n",
      "gram2-opposite\t\t33.99 %\n",
      "gram3-comparative\t\t85.59 %\n",
      "gram6-nationality-adjective\t\t88.33 %\n",
      "--- 58.367939472198486 seconds ---\n"
     ]
    }
   ],
   "source": [
    "print(\"GLOVE Model Accuacy on the Analogy Test Data\")\n",
    "pre_dict = preprocess_data(data, key_list, True)\n",
    "evaluation_dict = create_evaluation_dict(key_list)\n",
    "result = test_model(glove_model,pre_dict,evaluation_dict, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 8:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with some selected word to check the related word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_10_similar(model):\n",
    "    print('increase:\\n', model.most_similar('increase',topn=10), '\\n\\n')\n",
    "    print('enter:\\n' , model.most_similar('enter',topn=10) , '\\n\\n')\n",
    "    print('agree:\\n' , model.most_similar('agree',topn=10) , '\\n\\n')\n",
    "    print('glad:\\n' , model.most_similar('glad',topn=10) , '\\n\\n')\n",
    "    print('down:\\n' , model.most_similar('down',topn=10) , '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "increase:\n",
      " [('decrease', 0.8370318412780762), ('increases', 0.7709376811981201), ('increased', 0.7578042149543762), ('reduction', 0.6908220648765564), ('increasing', 0.6871615648269653), ('decreases', 0.6816173195838928), ('rise', 0.6352646946907043), ('decreasing', 0.621862530708313), ('decline', 0.6128641366958618), ('uptick', 0.5923734903335571)] \n",
      "\n",
      "\n",
      "enter:\n",
      " [('entering', 0.7399863004684448), ('entered', 0.6956064105033875), ('enters', 0.5622495412826538), ('entry', 0.551796019077301), ('Entering', 0.4845679700374603), ('participate', 0.48203885555267334), ('leave', 0.4764121174812317), ('join', 0.4726880192756653), ('register', 0.4570498466491699), ('go', 0.443431556224823)] \n",
      "\n",
      "\n",
      "agree:\n",
      " [('disagree', 0.7711759805679321), ('concur', 0.7131549119949341), ('agrees', 0.5929450988769531), ('disagreed', 0.5711544156074524), ('Agree', 0.5635050535202026), ('disagreeing', 0.5525434613227844), ('agreed', 0.5445383787155151), ('insist', 0.5273953080177307), ('accept', 0.5188775658607483), ('sympathize', 0.509772002696991)] \n",
      "\n",
      "\n",
      "glad:\n",
      " [('thankful', 0.7440484762191772), ('happy', 0.7408890724182129), ('grateful', 0.6907245516777039), ('thrilled', 0.6789539456367493), ('pleased', 0.6634493470191956), ('proud', 0.657325029373169), ('delighted', 0.6416580677032471), ('excited', 0.6197551488876343), ('sorry', 0.6091006994247437), ('overjoyed', 0.6063767671585083)] \n",
      "\n",
      "\n",
      "down:\n",
      " [('up', 0.6396992206573486), ('off', 0.6289231777191162), ('out', 0.6243277192115784), ('back', 0.5663232803344727), ('away', 0.5217265486717224), ('in.', 0.4786834120750427), ('around', 0.46412861347198486), ('Down', 0.46159279346466064), ('onto', 0.44676482677459717), ('into', 0.4266636371612549)] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get top 10 Similar word of the verbs using google word2vec\n",
    "top_10_similar(google_word2vec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "increase:\n",
      " [('increased', 0.8925613164901733), ('increases', 0.8847570419311523), ('increasing', 0.8761011958122253), ('decrease', 0.8753834962844849), ('reduce', 0.7716375589370728), ('significantly', 0.761581301689148), ('higher', 0.7435784339904785), ('decreased', 0.7423883676528931), ('reduced', 0.7417342662811279), ('improve', 0.7249066829681396)] \n",
      "\n",
      "\n",
      "enter:\n",
      " [('entering', 0.7882142066955566), ('entered', 0.7325311303138733), ('click', 0.6757079362869263), ('must', 0.6420891284942627), ('you', 0.6263235211372375), ('please', 0.6239920854568481), ('submit', 0.6233333349227905), ('choose', 0.614321231842041), ('wish', 0.6090442538261414), ('go', 0.6060982942581177)] \n",
      "\n",
      "\n",
      "agree:\n",
      " [('disagree', 0.8208084106445312), ('think', 0.7533746957778931), ('believe', 0.7479774951934814), ('agreed', 0.7399305105209351), ('say', 0.7305512428283691), ('assume', 0.7162786722183228), ('understand', 0.7144808769226074), ('would', 0.6996035575866699), ('obviously', 0.6992678046226501), ('honestly', 0.6975820660591125)] \n",
      "\n",
      "\n",
      "glad:\n",
      " [('hope', 0.7597659826278687), ('surprised', 0.7475764155387878), ('thank', 0.7414065599441528), (\"'m\", 0.740985631942749), ('thankful', 0.7389078736305237), ('happy', 0.7359830141067505), ('sorry', 0.734409511089325), ('i', 0.732394814491272), ('wish', 0.7314890623092651), ('pleased', 0.7284533977508545)] \n",
      "\n",
      "\n",
      "down:\n",
      " [('up', 0.8682929277420044), ('out', 0.8197706937789917), ('back', 0.7941277027130127), ('off', 0.7915661334991455), ('away', 0.7646163702011108), ('going', 0.7639520764350891), ('just', 0.7565957307815552), ('then', 0.755414605140686), ('put', 0.7499139308929443), ('go', 0.7497093677520752)] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Get top 10 Similar word of the verbs using glove\n",
    "top_10_similar(glove_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, we use increase, enter, agree, glad and down. In every case we found some opposite verbs with high cosign similarity score. <br>1. Increase with Decrease<br>2. Enter with Leave<br>3. Agree with Disagree<br>4. Glad with Sorry<br>5. Down with Up<br>Sometimes these opposite words are used in the same context in literature. For example, someone might be agree or disagree on the same topic. Word embedding takes into account with the neighbouring word and which context the word is using. That's why these opposite words gets the high cosign similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 9:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cause-effect analogy\n",
    "\n",
    "cause_effect_analogy = {'cause-effect':\n",
    "                            [\n",
    "                                {'a':'smile','b':'happy','c':'tear','d':'grief'},\n",
    "                                {'a':'sun','b':'heat','c':'wind','d':'breeze'},\n",
    "                                {'a':'heat','b':'drought','c':'rain','d':'flood'}\n",
    "                            ]\n",
    "                       }\n",
    "\n",
    "# object function analogy\n",
    "\n",
    "object_function_analogy = {'object-function':\n",
    "                            [\n",
    "                                {'a':'artist','b':'paint','c':'fish','d':'swim'},\n",
    "                                {'a':'bird','b':'fly','c':'beautician','d':'brush'},\n",
    "                                {'a':'soldier','b':'fight','c':'scientist','d':'research'}\n",
    "                            ]\n",
    "                          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using Word2Vec Model.\n",
      "\n",
      "Predicted: [('tearing', 0.49627578258514404)]\t\tTrue Value: grief\n",
      "Predicted: [('Wind', 0.49804264307022095)]\t\tTrue Value: breeze\n",
      "Predicted: [('dry_spell', 0.62548828125)]\t\tTrue Value: flood\n",
      "Accuracy By Analogy:\n",
      "cause-effect\t\t0.00 %\n",
      "--- 0.021989822387695312 seconds ---\n",
      "\n",
      "Using GLOVE Model.\n",
      "\n",
      "Predicted: [('break', 0.5146130323410034)]\t\tTrue Value: grief\n",
      "Predicted: [('turbine', 0.550868034362793)]\t\tTrue Value: breeze\n",
      "Predicted: [('rains', 0.6240835189819336)]\t\tTrue Value: flood\n",
      "Accuracy By Analogy:\n",
      "cause-effect\t\t0.00 %\n",
      "--- 0.020989418029785156 seconds ---\n"
     ]
    }
   ],
   "source": [
    "key_list = ['cause-effect']\n",
    "evaluation_dict = create_evaluation_dict(key_list)\n",
    "print('\\nUsing Word2Vec Model.\\n')\n",
    "result = test_model(google_word2vec_model,cause_effect_analogy,evaluation_dict, True)\n",
    "print('\\nUsing GLOVE Model.\\n')\n",
    "result = test_model(glove_model,cause_effect_analogy,evaluation_dict, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using Word2Vec Model.\n",
      "\n",
      "Predicted: [('catfish', 0.48208320140838623)]\t\tTrue Value: swim\n",
      "Predicted: [('hairdresser', 0.51582932472229)]\t\tTrue Value: brush\n",
      "Predicted: [('researcher', 0.456958532333374)]\t\tTrue Value: research\n",
      "Accuracy By Analogy:\n",
      "object-function\t\t0.00 %\n",
      "--- 0.02198934555053711 seconds ---\n",
      "\n",
      "Using GLOVE Model.\n",
      "\n",
      "Predicted: [('water', 0.5116518139839172)]\t\tTrue Value: swim\n",
      "Predicted: [('hairdresser', 0.5135518908500671)]\t\tTrue Value: brush\n",
      "Predicted: [('scientists', 0.5867975950241089)]\t\tTrue Value: research\n",
      "Accuracy By Analogy:\n",
      "object-function\t\t0.00 %\n",
      "--- 0.019990205764770508 seconds ---\n"
     ]
    }
   ],
   "source": [
    "key_list = ['object-function']\n",
    "evaluation_dict = create_evaluation_dict(key_list)\n",
    "print('\\nUsing Word2Vec Model.\\n')\n",
    "result = test_model(google_word2vec_model,object_function_analogy,evaluation_dict, True)\n",
    "print('\\nUsing GLOVE Model.\\n')\n",
    "result = test_model(glove_model,object_function_analogy,evaluation_dict, True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use cause-effect analogy and object-function analogy. Each cases the predicted word do not match. So, we found the 0 % accuracy. However, we obseved something simmilar word appeared, not the one we expected. For example, <b>soldier -> fight, researcher -> research</b>. We found <b>researchers</b> instead of <b>research</b>. So, somehow that's relatable. In another example, we require the output <b>swim</b>, but the result came out as <b>water</b>. In that case swim and water in pretty much relatable. So, we can find similar types of word in the analogy.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Analogy, https://penlighten.com/types-of-analogies-in-english-language<br>\n",
    "[2] Analogy, https://www.quia.com/jg/2500065list.html<br>\n",
    "[3] Scikit Learn, https://scikit-learn.org/stable/<br>\n",
    "[4] Word2Vec, https://code.google.com/archive/p/word2vec/<br>\n",
    "[5] GLOVE, https://nlp.stanford.edu/projects/glove/<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
