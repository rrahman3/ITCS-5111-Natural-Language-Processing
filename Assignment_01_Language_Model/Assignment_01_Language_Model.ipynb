{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1\n",
    "### NAME: RUHANI FAIHEEM RAHMAN\n",
    "### STUDENT ID: 801126769"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1(Language Model Creation)(80points).\n",
    "\n",
    "In  this  exercise,  you  will  train  probabilistic  language  models  to  distinguish between words in different languages. Rather than looking up whole words in a dictionary,  you  will  build  models  of  character  sequences  so  you  can  make  a guess about the language of unseen words. You will need to use NLTK and the Universal Declaration of Human Rights corpus.We  will  compare  across  different  languages  from  the  Universal  Declaration  of Human Rights documents. \n",
    "\n",
    "Use the following code to load the corpus and create sets of four languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.corpus import udhr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "english = udhr.raw('English-Latin1') \n",
    "french = udhr.raw('French_Francais-Latin1') \n",
    "italian = udhr.raw('Italian_Italiano-Latin1') \n",
    "spanish = udhr.raw('Spanish_Espanol-Latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_train, english_dev = english[0:1000], english[1000:1100] \n",
    "french_train, french_dev = french[0:1000], french[1000:1100]\n",
    "italian_train, italian_dev = italian[0:1000], italian[1000:1100] \n",
    "spanish_train, spanish_dev = spanish[0:1000], spanish[1000:1100]  \n",
    "\n",
    "english_test = udhr.words('English-Latin1')[0:1000] \n",
    "french_test = udhr.words('French_Francais-Latin1')[0:1000]\n",
    "italian_test = udhr.words('Italian_Italiano-Latin1')[0:1000] \n",
    "spanish_test = udhr.words('Spanish_Espanol-Latin1')[0:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test of get_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\"\"\"\n",
    "    This method takes input text, preprocess the text, tokenize it and returns the list of ngram.\n",
    "    \n",
    "    Output looks like this\n",
    "    [('u', 'n', 'i'), ('n', 'i', 'v'), ('i', 'v', 'e'), ('v', 'e', 'r'), ('e', 'r', 's'), ('r', 's', 'a'), ('s', 'a', 'l'), ('a', 'l', None), ...]\n",
    "    \n",
    "\"\"\"\n",
    "def get_chars(text,n):\n",
    "    text = text.lower()\n",
    "    chars = list()\n",
    "    a = word_tokenize(text)\n",
    "    for a_ in a:\n",
    "        b = list(nltk.ngrams(a_,n,pad_left='True',pad_right='True'))\n",
    "        for b_ in b:\n",
    "            chars.append(b_)\n",
    "    return chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test for get_chars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(None, None, 'u'),\n",
       " (None, 'u', 'n'),\n",
       " ('u', 'n', 'i'),\n",
       " ('n', 'i', 'v'),\n",
       " ('i', 'v', 'e'),\n",
       " ('v', 'e', 'r'),\n",
       " ('e', 'r', 's'),\n",
       " ('r', 's', 'a'),\n",
       " ('s', 'a', 'l'),\n",
       " ('a', 'l', None)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_chars(english_train,3)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    This helping method returns the pair list of the ngrams to prepare the data to use ConditionalFreqDist()\n",
    "    Fist pair, Second pair\n",
    "\"\"\"\n",
    "def get_pair_list(char_lists, n):\n",
    "    if n==1:\n",
    "        pair_lists = char_lists\n",
    "    elif n==2:\n",
    "        pair_lists = (((w0), w1) for w0, w1 in char_lists)\n",
    "    elif n==3:\n",
    "        pair_lists = (((w0, w1), w2) for w0, w1, w2 in char_lists)\n",
    "#     elif n==4:\n",
    "#         pair_lists = (((w0, w1, w2), w3) for w0, w1, w2, w3 in char_lists)\n",
    "#     elif n==5:\n",
    "#         pair_lists = (((w0, w1, w2, w3), w4) for w0, w1, w2, w3, w4 in char_lists)\n",
    "    else:\n",
    "        print('We are not there yet!!!')\n",
    "#    print(pair_lists)\n",
    "    return pair_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    This method returns the Frequency Distribution of the ngram list\n",
    "\"\"\"\n",
    "def get_distribution(pair_lists, n):\n",
    "    if n==1:\n",
    "        dist_lists = nltk.FreqDist(pair_lists)\n",
    "    else:\n",
    "        dist_lists = nltk.ConditionalFreqDist(pair_lists)\n",
    "    return dist_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    This method calculates the probability using add one smoothing formula\n",
    "    \n",
    "    For unigram, it uses simple N_gram rule to calculate probability\n",
    "    For bigram and trigram, this method uses add one smoothing\n",
    "    \n",
    "\"\"\"\n",
    "def predict_probabilities(word, model,help_model, n):\n",
    "    word_chars = get_chars(word.lower(),n)\n",
    "    word_ngrams  = list()\n",
    "\n",
    "    if n == 1:\n",
    "        for a in word_chars:\n",
    "            word_ngrams.append(a[0])\n",
    "    else:\n",
    "        word_ngrams = word_chars\n",
    "\n",
    "    probabilities = 1\n",
    "    v = len(word_ngrams)\n",
    "    \n",
    "    \"\"\"\n",
    "        This section implemtns the add one smoothing for the ngram.\n",
    "        \n",
    "        formula for probabilities = (1+count(current))/(total_word+count(previous))\n",
    "    \"\"\"\n",
    "    v = len(help_model)\n",
    "    t_len = 1000\n",
    "    for i in range(len(word_ngrams)):        \n",
    "        curr = word_ngrams[i]\n",
    "        \n",
    "        if n == 1:\n",
    "            key = 'None'\n",
    "        elif n == 2:\n",
    "            key = (curr[0])\n",
    "        elif n == 3:\n",
    "            key = (curr[0], curr[1])\n",
    "        try:\n",
    "            denomenator = help_model[key]\n",
    "        except:\n",
    "            denomenator = 0\n",
    "        try:\n",
    "            count = model[curr]\n",
    "        except:\n",
    "            count = 0\n",
    "\n",
    "        if n==1:\n",
    "            prob = (1+count)/(t_len)\n",
    "        else:\n",
    "            prob = (1.0+count)/(v+denomenator)\n",
    "        probabilities *= prob\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    This method calculated the ngram model. Returns Two list of Frequency Distribution.\n",
    "    First list contains the frequency distribution of n grams\n",
    "    Second list contains the frequency distribution of (n-1) grams\n",
    "    \n",
    "    If you call with n=3,\n",
    "        first list contains [('o', 'f', None): 14.0, ('f', None, None): 16.0, (None, 'h', 'u'): 7.0, (None, 'h', 'a'): 5.0, (None, 'h', 'i'): 1.0, ...]\n",
    "        second list contains [ ('e', 'a'): 9, ('a', 'm'): 3, ('m', 'b'): 2, ('b', 'l'): 2, ('l', 'e'): 6, ('e', None): 31, (None, 'w'): 11,]\n",
    "    \n",
    "\"\"\"\n",
    "def get_model(train_data, n):\n",
    "    total_chars = 1#len(train_data)\n",
    "    ### Get chars list\n",
    "    eng_chars = get_chars(train_data,n)\n",
    "    \n",
    "    ### get conditional pair list of char\n",
    "    eng_tri_gram_pairs = get_pair_list(eng_chars,n)\n",
    "    \n",
    "    ### frequency distribution list\n",
    "    eng_tri_gram_distribution = get_distribution(eng_tri_gram_pairs, n)\n",
    "    \n",
    "    eng_tri_gram_model = dict()\n",
    "    conditional_model = dict()\n",
    "\n",
    "    ### calculate model frequency for the training set\n",
    "    for key, val in eng_tri_gram_distribution.items():\n",
    "        if n==1:\n",
    "            eng_tri_gram_model[key[0]] = eng_tri_gram_distribution[key]/total_chars\n",
    "\n",
    "        elif n==2:\n",
    "            for values in val:\n",
    "                eng_tri_gram_model[(key,values)] = eng_tri_gram_distribution[key][values]/total_chars\n",
    "                try:\n",
    "                    conditional_model[key] = conditional_model[key] + eng_tri_gram_distribution[key][values]\n",
    "                except:\n",
    "                    conditional_model[key] = eng_tri_gram_distribution[key][values]\n",
    "                \n",
    "        elif n==3:\n",
    "            for values in val:\n",
    "                eng_tri_gram_model[(key[0],key[1],values)] = eng_tri_gram_distribution[key][values]/total_chars\n",
    "                try:\n",
    "                    conditional_model[key] = conditional_model[key] + eng_tri_gram_distribution[key][values]\n",
    "                except:\n",
    "                    conditional_model[key] = eng_tri_gram_distribution[key][values]\n",
    "#         elif n==4:\n",
    "#             for values in val:\n",
    "#                 eng_tri_gram_model[(key[0],key[1],key[2],values)] = eng_tri_gram_distribution[key][values]/total_chars\n",
    "#         elif n==5:\n",
    "#             for values in val:\n",
    "#                 eng_tri_gram_model[(key[0],key[1],key[2],key[3],values)] = eng_tri_gram_distribution[key][values]/total_chars\n",
    "#    print(eng_tri_gram_model)\n",
    "    return eng_tri_gram_model, conditional_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = get_model(english_train,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This method is the main method here. This method will take train data, test data and ngram as parameters.\n",
    "@parameters: two training data, two test data, ngram\n",
    "\"\"\"\n",
    "def model(train1, train2, test1, test2, n, des1, des2):\n",
    "    \"\"\"\n",
    "    get_model with provide the model from the training data set. It returns a dictionary\n",
    "\n",
    "    Dictionary looks like this\n",
    "\n",
    "    {('e', 'c') 5.0,('e', 'a') 9.0,('e', None) 31.0,('e', 'n') 12.0,('e', 'q') 1.0,('e', 'm') 2.0,('e', 'e') 6.0, ...}\n",
    "    \n",
    "    ngram is the key and prediction is the value.\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "        ### calculate model for both training data set\n",
    "    \"\"\"\n",
    "    english_trigram_model, help_model_1 = get_model(train1,n)\n",
    "    french_trigram_model, help_model_2 = get_model(train2,n)\n",
    "\n",
    "    \"\"\"\n",
    "        ### @pred_results stores the value for prediction\n",
    "    \"\"\"\n",
    "    pred_results = {'false-positive':0,'false-negetive':0,'true-positive':0,'true-negetive':0}\n",
    " \n",
    "    for words in test1:\n",
    "        \"\"\"\n",
    "            Calculate probailities for each model\n",
    "        \"\"\"\n",
    "        eng_pred = predict_probabilities(words, english_trigram_model,help_model_1,n)\n",
    "        french_pred = predict_probabilities(words, french_trigram_model,help_model_2,n)\n",
    "    \n",
    "        if (eng_pred >= french_pred):\n",
    "            pred_results['true-positive'] += 1\n",
    "        else:\n",
    "            pred_results['false-positive'] += 1\n",
    "    \"\"\"\n",
    "        ### Accuracy of the first model.\n",
    "    \"\"\"\n",
    "    print('Accuracy of the {} model for {} grams: {} %'.format(des1, n, pred_results['true-positive']/(pred_results['true-positive']+pred_results['false-positive']) * 100.0))\n",
    "\n",
    "#     pred_results = {'false-positive':0,'false-negetive':0,'true-positive':0,'true-negetive':0}\n",
    "\n",
    "#     for words in test2:\n",
    "#         eng_pred = predict_probabilities(words, english_trigram_model,help_model_1,n)\n",
    "#         french_pred = predict_probabilities(words, french_trigram_model,help_model_2,n)\n",
    "#         if (eng_pred > french_pred):\n",
    "#             pred_results['false-positive'] += 1\n",
    "#         else:\n",
    "#             pred_results['true-positive'] += 1\n",
    "\n",
    "#     \"\"\"\n",
    "#         ### Accuracy of the second model.\n",
    "#     \"\"\"\n",
    "#     print('Accuracy of the {} model for {} grams: {} %'.format(des2, n, pred_results['true-positive']/(pred_results['true-positive']+pred_results['false-positive']) * 100.0))\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### English Test Data with English vs Franch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the English model for 1 grams: 78.4 %\n",
      "Accuracy of the English model for 2 grams: 87.8 %\n",
      "Accuracy of the English model for 3 grams: 87.8 %\n"
     ]
    }
   ],
   "source": [
    "model(english_train, french_train, english_test, english_test, 1, 'English','French')\n",
    "model(english_train, french_train, english_test, english_test, 2, 'English','French')\n",
    "model(english_train, french_train, english_test, english_test, 3, 'English','French')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### French Test Data with English vs Franch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the French model for 1 grams: 76.1 %\n",
      "Accuracy of the French model for 2 grams: 64.60000000000001 %\n",
      "Accuracy of the French model for 3 grams: 69.0 %\n"
     ]
    }
   ],
   "source": [
    "model(french_train, english_train, french_test, french_test, 1, 'French', 'English')\n",
    "model(french_train, english_train, french_test, french_test, 2, 'French', 'English')\n",
    "model(french_train, english_train, french_test, french_test, 3, 'French', 'English')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Analysis\n",
    "\n",
    "We can see here Accuracy of unigram for testing in English Data gets us 78.4% accuracy.\n",
    "for BiGram, the accuracy reduces 87.8 %\n",
    "for TriGram, the accuracy is also 87.8 %\n",
    "\n",
    "On the other hand, The accuracy for French test data on French model is 76.1 for unigram, 64.6% for bigram and 69 % for trigram. Here unigram performances is not good at all. \n",
    "\n",
    "So, in this model testing, we can conclude that English can be easily distingusable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2** (Language Model Comparison) (20 points).\n",
    "\n",
    "Perform  the  same  experiment as  above for  Spanish  vs.  Italian.  Which  language pair is harder to distinguish?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Italian Test Data with Spanish vs Italian Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Italian model for 1 grams: 65.2 %\n",
      "Accuracy of the Italian model for 2 grams: 79.9 %\n",
      "Accuracy of the Italian model for 3 grams: 78.9 %\n"
     ]
    }
   ],
   "source": [
    "model(italian_train, spanish_train, italian_test, italian_test, 1, 'Italian', 'Spanish')\n",
    "model(italian_train, spanish_train, italian_test, italian_test, 2, 'Italian', 'Spanish')\n",
    "model(italian_train, spanish_train, italian_test, italian_test, 3, 'Italian', 'Spanish')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spanish Test Data with Spanish vs Italian Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Spanish model for 1 grams: 65.9 %\n",
      "Accuracy of the Spanish model for 2 grams: 61.7 %\n",
      "Accuracy of the Spanish model for 3 grams: 81.2 %\n"
     ]
    }
   ],
   "source": [
    "model(spanish_train, italian_train, spanish_test, spanish_test, 1, 'Spanish', 'Italian')\n",
    "model(spanish_train, italian_train, spanish_test, spanish_test, 2, 'Spanish', 'Italian')\n",
    "model(spanish_train, italian_train, spanish_test, spanish_test, 3, 'Spanish', 'Italian')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Analysis\n",
    "\n",
    "We can see here Accuracy of unigram for testing in Italian Data gets us 65.2 % accuracy.\n",
    "for BiGram, the accuracy reduces 79.9 %\n",
    "for TriGram, the accuracy is also 78.9 %\n",
    "\n",
    "On the other hand, The accuracy for Spanish test data on Spanish model is 65.9 for unigram, 61.7% for bigram and 81.2 % for trigram. Here unigram performances is not good at all. \n",
    "\n",
    "So, in this model testing, Trigram for Spanish beats Italian, However Unigram and Bigram in Italian Language is better here. Finally, we can conclude that Italian can be easily distingusable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Harder to Distinguishable\n",
    "\n",
    "Based on our experiment, overall, English vs French accuracy is much higher than the pair of Italian vs Spanish. So we can conclude here that Italian vs Spanish language is harder to distinguish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
